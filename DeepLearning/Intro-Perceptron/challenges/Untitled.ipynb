{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=pd.read_csv(\"./datasets2/Train/Train_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=pd.read_fwf(\"./datasets2/Train/data_description.txt\",sep=\" \",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain=Xtrain['SalePrice']\n",
    "Ytrain=Ytrain.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=Xtrain.drop(columns=['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5   6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6   7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7   8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8   9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9  10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "2         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "4         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "5         Lvl    AllPub  ...           0        0    NaN  MnPrv        Shed   \n",
       "6         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "7         Lvl    AllPub  ...           0        0    NaN    NaN        Shed   \n",
       "8         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "9         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      2    2008        WD         Normal  \n",
       "1       0      5    2007        WD         Normal  \n",
       "2       0      9    2008        WD         Normal  \n",
       "3       0      2    2006        WD        Abnorml  \n",
       "4       0     12    2008        WD         Normal  \n",
       "5     700     10    2009        WD         Normal  \n",
       "6       0      8    2007        WD         Normal  \n",
       "7     350     11    2009        WD         Normal  \n",
       "8       0      4    2008        WD        Abnorml  \n",
       "9       0      1    2008        WD         Normal  \n",
       "\n",
       "[10 rows x 80 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=Xtrain.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype,is_integer_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericdtype=[]\n",
    "stringdtype=[]\n",
    "\n",
    "for c in col:\n",
    "    if is_string_dtype(Xtrain[c]):\n",
    "        stringdtype.append(c)\n",
    "    else:\n",
    "        numericdtype.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stringdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numericdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df=Xtrain[numericdtype].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=Imputer(missing_values='NaN',strategy='mean',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=imp.fit_transform(numerical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 37)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalling values between 0-1\n",
    "\n",
    "ss=StandardScaler()\n",
    "ss.fit(X)\n",
    "X=ss.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df=Xtrain[stringdtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>ConLw</th>\n",
       "      <th>New</th>\n",
       "      <th>Oth</th>\n",
       "      <th>WD</th>\n",
       "      <th>Abnorml</th>\n",
       "      <th>AdjLand</th>\n",
       "      <th>Alloca</th>\n",
       "      <th>Family</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.730477</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>-2.201717e-01</td>\n",
       "      <td>-0.193683</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>1.048308</td>\n",
       "      <td>0.871322</td>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.602702</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.727328</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>4.948350e-01</td>\n",
       "      <td>-0.088808</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>2.193400</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>-0.450819</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>1.224214</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.724179</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>-7.717033e-02</td>\n",
       "      <td>0.061665</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>0.981347</td>\n",
       "      <td>0.822354</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.721029</td>\n",
       "      <td>0.298015</td>\n",
       "      <td>-4.585072e-01</td>\n",
       "      <td>-0.093368</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>-1.897985</td>\n",
       "      <td>-0.744628</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>-0.516933</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.717880</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>6.855035e-01</td>\n",
       "      <td>0.336164</td>\n",
       "      <td>1.362400</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>0.947866</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>1.331252</td>\n",
       "      <td>0.486169</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.714731</td>\n",
       "      <td>-0.169793</td>\n",
       "      <td>7.331706e-01</td>\n",
       "      <td>0.322940</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>0.713502</td>\n",
       "      <td>0.479576</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>0.662112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.711582</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>2.564995e-01</td>\n",
       "      <td>-0.044669</td>\n",
       "      <td>1.362400</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>1.081789</td>\n",
       "      <td>0.969258</td>\n",
       "      <td>0.435197</td>\n",
       "      <td>2.117638</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.708433</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>6.773904e-16</td>\n",
       "      <td>-0.017493</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>0.043890</td>\n",
       "      <td>-0.597723</td>\n",
       "      <td>0.730240</td>\n",
       "      <td>0.952303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.705283</td>\n",
       "      <td>-0.169793</td>\n",
       "      <td>-8.875113e-01</td>\n",
       "      <td>-0.406169</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>-1.362295</td>\n",
       "      <td>-1.723991</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>-1.010487</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.702134</td>\n",
       "      <td>3.104863</td>\n",
       "      <td>-9.351784e-01</td>\n",
       "      <td>-0.287615</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>-1.094451</td>\n",
       "      <td>-1.723991</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>0.934023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.698985</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>1.816390e-02</td>\n",
       "      <td>0.057105</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>-0.223955</td>\n",
       "      <td>-0.989469</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>1.059697</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.695836</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>7.331706e-01</td>\n",
       "      <td>0.123131</td>\n",
       "      <td>2.084636</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>1.115269</td>\n",
       "      <td>1.018226</td>\n",
       "      <td>0.981572</td>\n",
       "      <td>1.269914</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.692687</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>6.773904e-16</td>\n",
       "      <td>0.218339</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>-0.324397</td>\n",
       "      <td>-1.136373</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>0.673537</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.689538</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>1.019173e+00</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>1.148750</td>\n",
       "      <td>1.067195</td>\n",
       "      <td>1.090847</td>\n",
       "      <td>-1.010487</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.686388</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>6.773904e-16</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>-0.391358</td>\n",
       "      <td>-1.234310</td>\n",
       "      <td>0.577255</td>\n",
       "      <td>0.664397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.683239</td>\n",
       "      <td>-0.286745</td>\n",
       "      <td>-8.875113e-01</td>\n",
       "      <td>-0.406169</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>2.193400</td>\n",
       "      <td>-1.429257</td>\n",
       "      <td>0.773386</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>-1.010487</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.680090</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>6.773904e-16</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>1.292115</td>\n",
       "      <td>-0.056552</td>\n",
       "      <td>-0.744628</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>0.310226</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.676941</td>\n",
       "      <td>0.765823</td>\n",
       "      <td>1.134981e-01</td>\n",
       "      <td>0.019806</td>\n",
       "      <td>-1.526544</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>-0.156994</td>\n",
       "      <td>-0.891532</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>-1.010487</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.673792</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>-1.725046e-01</td>\n",
       "      <td>0.284638</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>-0.510455</td>\n",
       "      <td>1.081789</td>\n",
       "      <td>0.920290</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>0.465604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.670642</td>\n",
       "      <td>-0.871505</td>\n",
       "      <td>1.816390e-02</td>\n",
       "      <td>-0.274847</td>\n",
       "      <td>-0.804308</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>-0.458319</td>\n",
       "      <td>-0.989469</td>\n",
       "      <td>-0.581060</td>\n",
       "      <td>0.141138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1             2         3         4         5         6  \\\n",
       "0  -1.730477  0.064111 -2.201717e-01 -0.193683  0.640164 -0.510455  1.048308   \n",
       "1  -1.727328 -0.871505  4.948350e-01 -0.088808 -0.082072  2.193400  0.144332   \n",
       "2  -1.724179  0.064111 -7.717033e-02  0.061665  0.640164 -0.510455  0.981347   \n",
       "3  -1.721029  0.298015 -4.585072e-01 -0.093368  0.640164 -0.510455 -1.897985   \n",
       "4  -1.717880  0.064111  6.855035e-01  0.336164  1.362400 -0.510455  0.947866   \n",
       "5  -1.714731 -0.169793  7.331706e-01  0.322940 -0.804308 -0.510455  0.713502   \n",
       "6  -1.711582 -0.871505  2.564995e-01 -0.044669  1.362400 -0.510455  1.081789   \n",
       "7  -1.708433  0.064111  6.773904e-16 -0.017493  0.640164  0.390830  0.043890   \n",
       "8  -1.705283 -0.169793 -8.875113e-01 -0.406169  0.640164 -0.510455 -1.362295   \n",
       "9  -1.702134  3.104863 -9.351784e-01 -0.287615 -0.804308  0.390830 -1.094451   \n",
       "10 -1.698985 -0.871505  1.816390e-02  0.057105 -0.804308 -0.510455 -0.223955   \n",
       "11 -1.695836  0.064111  7.331706e-01  0.123131  2.084636 -0.510455  1.115269   \n",
       "12 -1.692687 -0.871505  6.773904e-16  0.218339 -0.804308  0.390830 -0.324397   \n",
       "13 -1.689538 -0.871505  1.019173e+00  0.007130  0.640164 -0.510455  1.148750   \n",
       "14 -1.686388 -0.871505  6.773904e-16  0.031570 -0.082072 -0.510455 -0.391358   \n",
       "15 -1.683239 -0.286745 -8.875113e-01 -0.406169  0.640164  2.193400 -1.429257   \n",
       "16 -1.680090 -0.871505  6.773904e-16  0.060844 -0.082072  1.292115 -0.056552   \n",
       "17 -1.676941  0.765823  1.134981e-01  0.019806 -1.526544 -0.510455 -0.156994   \n",
       "18 -1.673792 -0.871505 -1.725046e-01  0.284638 -0.804308 -0.510455  1.081789   \n",
       "19 -1.670642 -0.871505  1.816390e-02 -0.274847 -0.804308  0.390830 -0.458319   \n",
       "\n",
       "           7         8         9  ...  ConLw  New  Oth  WD  Abnorml  AdjLand  \\\n",
       "0   0.871322  0.489835  0.602702  ...      0    0    0   1        0        0   \n",
       "1  -0.450819 -0.581060  1.224214  ...      0    0    0   1        0        0   \n",
       "2   0.822354  0.304067  0.100009  ...      0    0    0   1        0        0   \n",
       "3  -0.744628 -0.581060 -0.516933  ...      0    0    0   1        1        0   \n",
       "4   0.724417  1.331252  0.486169  ...      0    0    0   1        0        0   \n",
       "5   0.479576 -0.581060  0.662112  ...      0    0    0   1        0        0   \n",
       "6   0.969258  0.435197  2.117638  ...      0    0    0   1        0        0   \n",
       "7  -0.597723  0.730240  0.952303  ...      0    0    0   1        0        0   \n",
       "8  -1.723991 -0.581060 -1.010487  ...      0    0    0   1        1        0   \n",
       "9  -1.723991 -0.581060  0.934023  ...      0    0    0   1        0        0   \n",
       "10 -0.989469 -0.581060  1.059697  ...      0    0    0   1        0        0   \n",
       "11  1.018226  0.981572  1.269914  ...      0    1    0   0        0        0   \n",
       "12 -1.136373 -0.581060  0.673537  ...      0    0    0   1        0        0   \n",
       "13  1.067195  1.090847 -1.010487  ...      0    1    0   0        0        0   \n",
       "14 -1.234310  0.577255  0.664397  ...      0    0    0   1        0        0   \n",
       "15  0.773386 -0.581060 -1.010487  ...      0    0    0   1        0        0   \n",
       "16 -0.744628  0.402415  0.310226  ...      0    0    0   1        0        0   \n",
       "17 -0.891532 -0.581060 -1.010487  ...      0    0    0   1        0        0   \n",
       "18  0.920290 -0.581060  0.465604  ...      0    0    0   1        0        0   \n",
       "19 -0.989469 -0.581060  0.141138  ...      0    0    0   0        1        0   \n",
       "\n",
       "    Alloca  Family  Normal  Partial  \n",
       "0        0       0       1        0  \n",
       "1        0       0       1        0  \n",
       "2        0       0       1        0  \n",
       "3        0       0       0        0  \n",
       "4        0       0       1        0  \n",
       "5        0       0       1        0  \n",
       "6        0       0       1        0  \n",
       "7        0       0       1        0  \n",
       "8        0       0       0        0  \n",
       "9        0       0       1        0  \n",
       "10       0       0       1        0  \n",
       "11       0       0       0        1  \n",
       "12       0       0       1        0  \n",
       "13       0       0       0        1  \n",
       "14       0       0       1        0  \n",
       "15       0       0       1        0  \n",
       "16       0       0       1        0  \n",
       "17       0       0       1        0  \n",
       "18       0       0       1        0  \n",
       "19       0       0       0        0  \n",
       "\n",
       "[20 rows x 278 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_categorical(ndf, df, categorical_features):\n",
    "    for f in categorical_features:\n",
    "        new_cols = pd.DataFrame(pd.get_dummies(df[f]))\n",
    "        new_cols.index = df.index\n",
    "        ndf = pd.merge(ndf, new_cols, how = 'inner', left_index=True, right_index=True)\n",
    "    return ndf\n",
    "\n",
    "numeric_df = pd.DataFrame(X)\n",
    "numeric_df.index = Xtrain.index\n",
    "combined_df = process_categorical(numeric_df, Xtrain, string_df)\n",
    "combined_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=combined_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential()\n",
    "model.add(Dense(10,activation='relu',input_dim=X.shape[1]))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 10)                380       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 765\n",
      "Trainable params: 765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 880 samples, validate on 220 samples\n",
      "Epoch 1/500\n",
      "880/880 [==============================] - 0s 390us/step - loss: 40109118929.4545 - accuracy: 0.0000e+00 - val_loss: 35467273681.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40109034328.4364 - accuracy: 0.0000e+00 - val_loss: 35467213954.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40108954270.2545 - accuracy: 0.0000e+00 - val_loss: 35467151844.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40108872573.6727 - accuracy: 0.0000e+00 - val_loss: 35467090404.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40108789983.4182 - accuracy: 0.0000e+00 - val_loss: 35467028293.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40108707542.1091 - accuracy: 0.0000e+00 - val_loss: 35466954565.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40108614749.0909 - accuracy: 0.0000e+00 - val_loss: 35466874358.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40108510934.1091 - accuracy: 0.0000e+00 - val_loss: 35466786629.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      "880/880 [==============================] - 0s 37us/step - loss: 40108388947.7818 - accuracy: 0.0000e+00 - val_loss: 35466681176.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40108246109.0909 - accuracy: 0.0000e+00 - val_loss: 35466562392.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 40108077056.0000 - accuracy: 0.0000e+00 - val_loss: 35466424841.3091 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40107872553.8909 - accuracy: 0.0000e+00 - val_loss: 35466262714.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40107639528.7273 - accuracy: 0.0000e+00 - val_loss: 35466071244.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40107354149.2364 - accuracy: 0.0000e+00 - val_loss: 35465851103.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40107007106.3273 - accuracy: 0.0000e+00 - val_loss: 35465586948.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40106587601.4545 - accuracy: 0.0000e+00 - val_loss: 35465272971.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40106091538.6182 - accuracy: 0.0000e+00 - val_loss: 35464891075.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40105505438.2545 - accuracy: 0.0000e+00 - val_loss: 35464444685.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 40104783350.6909 - accuracy: 0.0000e+00 - val_loss: 35463919131.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40103967055.1273 - accuracy: 0.0000e+00 - val_loss: 35463303838.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 40102990568.7273 - accuracy: 0.0000e+00 - val_loss: 35462602528.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40101891202.3273 - accuracy: 0.0000e+00 - val_loss: 35461787573.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40100609675.6364 - accuracy: 0.0000e+00 - val_loss: 35460868877.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40099147776.0000 - accuracy: 0.0000e+00 - val_loss: 35459833483.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 40097500513.7455 - accuracy: 0.0000e+00 - val_loss: 35458672826.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40095713838.5455 - accuracy: 0.0000e+00 - val_loss: 35457360989.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/500\n",
      "880/880 [==============================] - 0s 38us/step - loss: 40093714692.6545 - accuracy: 0.0000e+00 - val_loss: 35455906164.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40091348917.5273 - accuracy: 0.0000e+00 - val_loss: 35454340077.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 40088914031.7091 - accuracy: 0.0000e+00 - val_loss: 35452596521.8909 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40086277399.2727 - accuracy: 0.0000e+00 - val_loss: 35450664252.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/500\n",
      "880/880 [==============================] - 0s 38us/step - loss: 40083233252.0727 - accuracy: 0.0000e+00 - val_loss: 35448610816.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 40080058032.8727 - accuracy: 0.0000e+00 - val_loss: 35446359430.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40076573751.8545 - accuracy: 0.0000e+00 - val_loss: 35443911140.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40072793367.2727 - accuracy: 0.0000e+00 - val_loss: 35441271379.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40068659758.5455 - accuracy: 0.0000e+00 - val_loss: 35438429574.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 40064288879.7091 - accuracy: 0.0000e+00 - val_loss: 35435334190.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 40059489484.8000 - accuracy: 0.0000e+00 - val_loss: 35432024845.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40054396369.4545 - accuracy: 0.0000e+00 - val_loss: 35428480018.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40048916517.2364 - accuracy: 0.0000e+00 - val_loss: 35424693229.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40042995339.6364 - accuracy: 0.0000e+00 - val_loss: 35420704395.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 40036882320.2909 - accuracy: 0.0000e+00 - val_loss: 35416421394.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 40030185360.2909 - accuracy: 0.0000e+00 - val_loss: 35411911773.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 40023302367.4182 - accuracy: 0.0000e+00 - val_loss: 35407092940.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 40015940738.3273 - accuracy: 0.0000e+00 - val_loss: 35401999378.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 40008119240.1455 - accuracy: 0.0000e+00 - val_loss: 35396652609.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 39999961125.2364 - accuracy: 0.0000e+00 - val_loss: 35391018449.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39991086955.0545 - accuracy: 0.0000e+00 - val_loss: 35385185000.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 32us/step - loss: 39982261936.8727 - accuracy: 0.0000e+00 - val_loss: 35378931078.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39972753668.6545 - accuracy: 0.0000e+00 - val_loss: 35372407193.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 39962845854.2545 - accuracy: 0.0000e+00 - val_loss: 35365568660.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 39952319432.1455 - accuracy: 0.0000e+00 - val_loss: 35358473122.9091 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39941187025.4545 - accuracy: 0.0000e+00 - val_loss: 35351146198.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 39930107047.5636 - accuracy: 0.0000e+00 - val_loss: 35343347265.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 39918001282.3273 - accuracy: 0.0000e+00 - val_loss: 35335267421.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39905443765.5273 - accuracy: 0.0000e+00 - val_loss: 35326821394.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 39892548589.3818 - accuracy: 0.0000e+00 - val_loss: 35317934787.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39878967147.0545 - accuracy: 0.0000e+00 - val_loss: 35308664571.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/500\n",
      "880/880 [==============================] - 0s 28us/step - loss: 39865422792.1455 - accuracy: 0.0000e+00 - val_loss: 35298820319.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39850174873.6000 - accuracy: 0.0000e+00 - val_loss: 35288861751.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39834920252.5091 - accuracy: 0.0000e+00 - val_loss: 35278574461.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39819070817.7455 - accuracy: 0.0000e+00 - val_loss: 35267903636.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 39803488181.5273 - accuracy: 0.0000e+00 - val_loss: 35256571717.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39786328659.7818 - accuracy: 0.0000e+00 - val_loss: 35245002230.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 39768489984.0000 - accuracy: 0.0000e+00 - val_loss: 35233211261.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/500\n",
      "880/880 [==============================] - 0s 28us/step - loss: 39750339267.4909 - accuracy: 0.0000e+00 - val_loss: 35221065355.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39731591354.1818 - accuracy: 0.0000e+00 - val_loss: 35208515732.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39712410977.7455 - accuracy: 0.0000e+00 - val_loss: 35195485239.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 39692759933.6727 - accuracy: 0.0000e+00 - val_loss: 35181906999.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39672200992.5818 - accuracy: 0.0000e+00 - val_loss: 35167914765.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39651268608.0000 - accuracy: 0.0000e+00 - val_loss: 35153448811.0545 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39629148606.8364 - accuracy: 0.0000e+00 - val_loss: 35138739181.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39606732911.7091 - accuracy: 0.0000e+00 - val_loss: 35123606025.3091 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/500\n",
      "880/880 [==============================] - ETA: 0s - loss: 41240932352.0000 - accuracy: 0.0000e+ - 0s 28us/step - loss: 39584133194.4727 - accuracy: 0.0000e+00 - val_loss: 35107850053.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39560384065.1636 - accuracy: 0.0000e+00 - val_loss: 35091729538.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39535777307.9273 - accuracy: 0.0000e+00 - val_loss: 35075269501.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39511171481.6000 - accuracy: 0.0000e+00 - val_loss: 35058174287.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39484806609.4545 - accuracy: 0.0000e+00 - val_loss: 35040989332.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39458939699.2000 - accuracy: 0.0000e+00 - val_loss: 35023118410.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39431616549.2364 - accuracy: 0.0000e+00 - val_loss: 35004903796.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39404727128.4364 - accuracy: 0.0000e+00 - val_loss: 34985851364.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39375838189.3818 - accuracy: 0.0000e+00 - val_loss: 34966540213.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/500\n",
      "880/880 [==============================] - 0s 27us/step - loss: 39346657391.7091 - accuracy: 0.0000e+00 - val_loss: 34946837932.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39316868896.5818 - accuracy: 0.0000e+00 - val_loss: 34926637353.8909 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 39285960852.9455 - accuracy: 0.0000e+00 - val_loss: 34906170907.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39254577524.3636 - accuracy: 0.0000e+00 - val_loss: 34885167290.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 39222872920.4364 - accuracy: 0.0000e+00 - val_loss: 34863481055.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/500\n",
      "880/880 [==============================] - 0s 28us/step - loss: 39191272615.5636 - accuracy: 0.0000e+00 - val_loss: 34840883944.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/500\n",
      "880/880 [==============================] - 0s 28us/step - loss: 39157645163.0545 - accuracy: 0.0000e+00 - val_loss: 34817990209.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39122055242.4727 - accuracy: 0.0000e+00 - val_loss: 34795249440.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39088172795.3455 - accuracy: 0.0000e+00 - val_loss: 34771498449.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 39052247040.0000 - accuracy: 0.0000e+00 - val_loss: 34747448152.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 39015283469.9636 - accuracy: 0.0000e+00 - val_loss: 34723097804.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 38978569755.9273 - accuracy: 0.0000e+00 - val_loss: 34697965642.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38941575205.2364 - accuracy: 0.0000e+00 - val_loss: 34671926029.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 35us/step - loss: 38901944841.3091 - accuracy: 0.0000e+00 - val_loss: 34645772697.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 38863653906.6182 - accuracy: 0.0000e+00 - val_loss: 34618704393.3091 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 38822307058.0364 - accuracy: 0.0000e+00 - val_loss: 34591715030.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38781244211.2000 - accuracy: 0.0000e+00 - val_loss: 34564360005.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 38740564824.4364 - accuracy: 0.0000e+00 - val_loss: 34535911572.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 38697549079.2727 - accuracy: 0.0000e+00 - val_loss: 34507246349.9636 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 38654559474.0364 - accuracy: 0.0000e+00 - val_loss: 34477910872.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38609897062.4000 - accuracy: 0.0000e+00 - val_loss: 34448368584.1455 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 38565652815.1273 - accuracy: 0.0000e+00 - val_loss: 34418022884.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38521051917.9636 - accuracy: 0.0000e+00 - val_loss: 34386751041.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38473120823.8545 - accuracy: 0.0000e+00 - val_loss: 34355679567.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 38426958159.1273 - accuracy: 0.0000e+00 - val_loss: 34323714941.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 38379480678.4000 - accuracy: 0.0000e+00 - val_loss: 34291128171.0545 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 38330184275.7818 - accuracy: 0.0000e+00 - val_loss: 34258181697.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 38280735353.0182 - accuracy: 0.0000e+00 - val_loss: 34224674331.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38230711500.8000 - accuracy: 0.0000e+00 - val_loss: 34190616091.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 38177714176.0000 - accuracy: 0.0000e+00 - val_loss: 34156812288.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 38128887137.7455 - accuracy: 0.0000e+00 - val_loss: 34120943839.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 38074895117.9636 - accuracy: 0.0000e+00 - val_loss: 34085256992.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 38021453563.3455 - accuracy: 0.0000e+00 - val_loss: 34048909125.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37966654892.2182 - accuracy: 0.0000e+00 - val_loss: 34012334452.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37912923936.5818 - accuracy: 0.0000e+00 - val_loss: 33974496684.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 37854976335.1273 - accuracy: 0.0000e+00 - val_loss: 33937075367.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37798281737.3091 - accuracy: 0.0000e+00 - val_loss: 33898955608.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 37742490568.1455 - accuracy: 0.0000e+00 - val_loss: 33859469870.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 37682696415.4182 - accuracy: 0.0000e+00 - val_loss: 33820101464.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 37624283955.2000 - accuracy: 0.0000e+00 - val_loss: 33779955339.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 37563695476.3636 - accuracy: 0.0000e+00 - val_loss: 33739394699.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 37504367746.3273 - accuracy: 0.0000e+00 - val_loss: 33697766381.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37441018712.4364 - accuracy: 0.0000e+00 - val_loss: 33656436959.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37379248425.8909 - accuracy: 0.0000e+00 - val_loss: 33614215540.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 37316288698.1818 - accuracy: 0.0000e+00 - val_loss: 33571449911.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 37252586700.8000 - accuracy: 0.0000e+00 - val_loss: 33527943540.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37186917841.4545 - accuracy: 0.0000e+00 - val_loss: 33484246593.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 37121573050.1818 - accuracy: 0.0000e+00 - val_loss: 33439894323.2000 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 37055563924.9455 - accuracy: 0.0000e+00 - val_loss: 33394944707.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 36988961363.7818 - accuracy: 0.0000e+00 - val_loss: 33349187397.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 36920247314.6182 - accuracy: 0.0000e+00 - val_loss: 33303263567.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 36851211990.1091 - accuracy: 0.0000e+00 - val_loss: 33256705228.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 36781373179.3455 - accuracy: 0.0000e+00 - val_loss: 33209655035.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 36711216370.0364 - accuracy: 0.0000e+00 - val_loss: 33161776258.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 36639712498.0364 - accuracy: 0.0000e+00 - val_loss: 33113360309.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 36569002840.4364 - accuracy: 0.0000e+00 - val_loss: 33063934771.2000 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 36493133786.7636 - accuracy: 0.0000e+00 - val_loss: 33015322363.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 36420317854.2545 - accuracy: 0.0000e+00 - val_loss: 32965618334.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 36348104778.4727 - accuracy: 0.0000e+00 - val_loss: 32914352686.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 36270247116.8000 - accuracy: 0.0000e+00 - val_loss: 32863590586.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 32us/step - loss: 36193962542.5455 - accuracy: 0.0000e+00 - val_loss: 32812098169.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 36118712915.7818 - accuracy: 0.0000e+00 - val_loss: 32759368722.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 36040446696.7273 - accuracy: 0.0000e+00 - val_loss: 32706314984.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 35960782252.2182 - accuracy: 0.0000e+00 - val_loss: 32653107088.2909 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 35882257538.3273 - accuracy: 0.0000e+00 - val_loss: 32599124526.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 35801895433.3091 - accuracy: 0.0000e+00 - val_loss: 32544787176.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 35721775066.7636 - accuracy: 0.0000e+00 - val_loss: 32489406166.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 35639727420.5091 - accuracy: 0.0000e+00 - val_loss: 32433655919.7091 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 35555879247.1273 - accuracy: 0.0000e+00 - val_loss: 32378036633.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 35473663515.9273 - accuracy: 0.0000e+00 - val_loss: 32321622388.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 35391369476.6545 - accuracy: 0.0000e+00 - val_loss: 32263943372.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 35305544908.8000 - accuracy: 0.0000e+00 - val_loss: 32206365044.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 35220826000.2909 - accuracy: 0.0000e+00 - val_loss: 32148092741.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 35133620596.3636 - accuracy: 0.0000e+00 - val_loss: 32089705565.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 35048051321.0182 - accuracy: 0.0000e+00 - val_loss: 32030291930.7636 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 34960377483.6364 - accuracy: 0.0000e+00 - val_loss: 31970514162.0364 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 34869922648.4364 - accuracy: 0.0000e+00 - val_loss: 31911386912.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 34782105823.4182 - accuracy: 0.0000e+00 - val_loss: 31851098558.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 34692675621.2364 - accuracy: 0.0000e+00 - val_loss: 31790146578.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 34603371017.3091 - accuracy: 0.0000e+00 - val_loss: 31728129750.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 34512505725.6727 - accuracy: 0.0000e+00 - val_loss: 31665460838.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 34418920466.6182 - accuracy: 0.0000e+00 - val_loss: 31603355387.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 34326949143.2727 - accuracy: 0.0000e+00 - val_loss: 31540308377.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 34231694801.4545 - accuracy: 0.0000e+00 - val_loss: 31477562833.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 34143152723.7818 - accuracy: 0.0000e+00 - val_loss: 31411798388.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 34043123302.4000 - accuracy: 0.0000e+00 - val_loss: 31348085182.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 33948878587.3455 - accuracy: 0.0000e+00 - val_loss: 31282950590.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 33852543236.6545 - accuracy: 0.0000e+00 - val_loss: 31217389381.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 33757871159.8545 - accuracy: 0.0000e+00 - val_loss: 31150437804.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 33658392427.0545 - accuracy: 0.0000e+00 - val_loss: 31083984821.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 33560294716.5091 - accuracy: 0.0000e+00 - val_loss: 31016847434.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 33460664394.4727 - accuracy: 0.0000e+00 - val_loss: 30949710941.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 33362832141.9636 - accuracy: 0.0000e+00 - val_loss: 30881526839.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 33262982907.3455 - accuracy: 0.0000e+00 - val_loss: 30812884731.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 33162653696.0000 - accuracy: 0.0000e+00 - val_loss: 30743522322.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 33060462480.2909 - accuracy: 0.0000e+00 - val_loss: 30674114969.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 32955895659.0545 - accuracy: 0.0000e+00 - val_loss: 30605500192.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 32854748625.4545 - accuracy: 0.0000e+00 - val_loss: 30535419717.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 32752774870.1091 - accuracy: 0.0000e+00 - val_loss: 30464148870.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 32647876272.8727 - accuracy: 0.0000e+00 - val_loss: 30392725206.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 32544828211.2000 - accuracy: 0.0000e+00 - val_loss: 30320221202.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 32436992633.0182 - accuracy: 0.0000e+00 - val_loss: 30248545931.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 32330975120.2909 - accuracy: 0.0000e+00 - val_loss: 30176265867.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 32224660051.7818 - accuracy: 0.0000e+00 - val_loss: 30103453733.2364 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 32118487710.2545 - accuracy: 0.0000e+00 - val_loss: 30029834128.2909 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 32011135348.3636 - accuracy: 0.0000e+00 - val_loss: 29955773979.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 31902899330.3273 - accuracy: 0.0000e+00 - val_loss: 29881515547.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 34us/step - loss: 31793609970.0364 - accuracy: 0.0000e+00 - val_loss: 29806650721.7455 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 31686351369.3091 - accuracy: 0.0000e+00 - val_loss: 29730450748.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 31572120408.4364 - accuracy: 0.0000e+00 - val_loss: 29655892005.2364 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 31465250741.5273 - accuracy: 0.0000e+00 - val_loss: 29579563194.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 31353671419.3455 - accuracy: 0.0000e+00 - val_loss: 29503171788.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 31240501694.8364 - accuracy: 0.0000e+00 - val_loss: 29427069896.1455 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/500\n",
      "880/880 [==============================] - 0s 38us/step - loss: 31129568758.6909 - accuracy: 0.0000e+00 - val_loss: 29350021194.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 31016409162.4727 - accuracy: 0.0000e+00 - val_loss: 29272884950.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 30905526123.0545 - accuracy: 0.0000e+00 - val_loss: 29194500468.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 30790870891.0545 - accuracy: 0.0000e+00 - val_loss: 29116399951.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/500\n",
      "880/880 [==============================] - 0s 27us/step - loss: 30675541271.2727 - accuracy: 0.0000e+00 - val_loss: 29038421457.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 30565040537.6000 - accuracy: 0.0000e+00 - val_loss: 28958462622.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 30446030289.4545 - accuracy: 0.0000e+00 - val_loss: 28880026530.9091 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 30332794246.9818 - accuracy: 0.0000e+00 - val_loss: 28800082068.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 30217043632.8727 - accuracy: 0.0000e+00 - val_loss: 28719935190.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 30098562997.5273 - accuracy: 0.0000e+00 - val_loss: 28640365177.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/500\n",
      "880/880 [==============================] - 0s 40us/step - loss: 29983629907.7818 - accuracy: 0.0000e+00 - val_loss: 28559214740.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 29866403914.4727 - accuracy: 0.0000e+00 - val_loss: 28477748801.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 29748411112.7273 - accuracy: 0.0000e+00 - val_loss: 28396234826.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 29630920070.9818 - accuracy: 0.0000e+00 - val_loss: 28314294756.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 29510140127.4182 - accuracy: 0.0000e+00 - val_loss: 28232977650.0364 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 29395554006.1091 - accuracy: 0.0000e+00 - val_loss: 28149390987.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 29271896697.0182 - accuracy: 0.0000e+00 - val_loss: 28067566163.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 29153976878.5455 - accuracy: 0.0000e+00 - val_loss: 27984534770.0364 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 29033820569.6000 - accuracy: 0.0000e+00 - val_loss: 27901238104.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 28914995795.7818 - accuracy: 0.0000e+00 - val_loss: 27817170720.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 28792863688.1455 - accuracy: 0.0000e+00 - val_loss: 27733454587.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 28672435516.5091 - accuracy: 0.0000e+00 - val_loss: 27649299921.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 28550436454.4000 - accuracy: 0.0000e+00 - val_loss: 27565112636.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 28427692925.6727 - accuracy: 0.0000e+00 - val_loss: 27481264872.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 28307832534.1091 - accuracy: 0.0000e+00 - val_loss: 27395805370.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 28183394229.5273 - accuracy: 0.0000e+00 - val_loss: 27311206102.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 28062915974.9818 - accuracy: 0.0000e+00 - val_loss: 27224998260.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 27941039420.5091 - accuracy: 0.0000e+00 - val_loss: 27138281174.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 27815419941.2364 - accuracy: 0.0000e+00 - val_loss: 27052536403.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 27690367422.8364 - accuracy: 0.0000e+00 - val_loss: 26967383430.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 27569616486.4000 - accuracy: 0.0000e+00 - val_loss: 26880650482.0364 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 27442663721.8909 - accuracy: 0.0000e+00 - val_loss: 26794929021.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 27320062622.2545 - accuracy: 0.0000e+00 - val_loss: 26708102162.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 27196820424.1455 - accuracy: 0.0000e+00 - val_loss: 26620417824.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 27073817953.7455 - accuracy: 0.0000e+00 - val_loss: 26531797699.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 26943988382.2545 - accuracy: 0.0000e+00 - val_loss: 26445913814.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 26821006466.3273 - accuracy: 0.0000e+00 - val_loss: 26358726656.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 26698024326.9818 - accuracy: 0.0000e+00 - val_loss: 26270188115.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 26568669184.0000 - accuracy: 0.0000e+00 - val_loss: 26182904403.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 26446705719.8545 - accuracy: 0.0000e+00 - val_loss: 26093983110.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 26318333467.9273 - accuracy: 0.0000e+00 - val_loss: 26005796491.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 30us/step - loss: 26195083859.7818 - accuracy: 0.0000e+00 - val_loss: 25916186251.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 26065015677.6727 - accuracy: 0.0000e+00 - val_loss: 25828923094.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 25941837153.7455 - accuracy: 0.0000e+00 - val_loss: 25740062645.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 25813161835.0545 - accuracy: 0.0000e+00 - val_loss: 25651942753.7455 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 25688452263.5636 - accuracy: 0.0000e+00 - val_loss: 25562656283.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 25562189228.2182 - accuracy: 0.0000e+00 - val_loss: 25472986428.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 25437581163.0545 - accuracy: 0.0000e+00 - val_loss: 25382443901.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 25311484425.3091 - accuracy: 0.0000e+00 - val_loss: 25291837030.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 25180619012.6545 - accuracy: 0.0000e+00 - val_loss: 25203428854.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 25055397217.7455 - accuracy: 0.0000e+00 - val_loss: 25114419572.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 24931312360.7273 - accuracy: 0.0000e+00 - val_loss: 25024040848.2909 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 24804504408.4364 - accuracy: 0.0000e+00 - val_loss: 24933996655.7091 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 24677822501.2364 - accuracy: 0.0000e+00 - val_loss: 24844424992.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 24551110395.3455 - accuracy: 0.0000e+00 - val_loss: 24754892613.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 24423474306.3273 - accuracy: 0.0000e+00 - val_loss: 24666102169.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 24297431188.9455 - accuracy: 0.0000e+00 - val_loss: 24577182813.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 24171510281.3091 - accuracy: 0.0000e+00 - val_loss: 24488003025.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 24047138704.2909 - accuracy: 0.0000e+00 - val_loss: 24397880413.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 23920964775.5636 - accuracy: 0.0000e+00 - val_loss: 24307925438.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 23792284355.4909 - accuracy: 0.0000e+00 - val_loss: 24218901113.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 23668204488.1455 - accuracy: 0.0000e+00 - val_loss: 24128732699.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 23542136347.9273 - accuracy: 0.0000e+00 - val_loss: 24038345858.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 23417177776.8727 - accuracy: 0.0000e+00 - val_loss: 23947302278.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 23288577638.4000 - accuracy: 0.0000e+00 - val_loss: 23857476030.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 23163882142.2545 - accuracy: 0.0000e+00 - val_loss: 23767502996.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 23037827853.9636 - accuracy: 0.0000e+00 - val_loss: 23677443276.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 22913705760.5818 - accuracy: 0.0000e+00 - val_loss: 23587038189.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 22787608613.2364 - accuracy: 0.0000e+00 - val_loss: 23496758774.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 22660377320.7273 - accuracy: 0.0000e+00 - val_loss: 23407683509.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 22536015574.1091 - accuracy: 0.0000e+00 - val_loss: 23317999169.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 22412688141.9636 - accuracy: 0.0000e+00 - val_loss: 23227701918.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 22286511792.8727 - accuracy: 0.0000e+00 - val_loss: 23138175683.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 268/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 22162636315.9273 - accuracy: 0.0000e+00 - val_loss: 23048744103.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 22039704557.3818 - accuracy: 0.0000e+00 - val_loss: 22958553274.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 21913478944.5818 - accuracy: 0.0000e+00 - val_loss: 22869498079.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 21789845317.8182 - accuracy: 0.0000e+00 - val_loss: 22780226429.6727 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 21667089947.9273 - accuracy: 0.0000e+00 - val_loss: 22690236192.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 21543368890.1818 - accuracy: 0.0000e+00 - val_loss: 22600428562.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 21421908079.7091 - accuracy: 0.0000e+00 - val_loss: 22509827016.1455 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/500\n",
      "880/880 [==============================] - 0s 29us/step - loss: 21295213735.5636 - accuracy: 0.0000e+00 - val_loss: 22420939198.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 21174343680.0000 - accuracy: 0.0000e+00 - val_loss: 22330668646.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 21048767990.6909 - accuracy: 0.0000e+00 - val_loss: 22241661858.9091 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 20928056338.6182 - accuracy: 0.0000e+00 - val_loss: 22151420723.2000 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 20802695279.7091 - accuracy: 0.0000e+00 - val_loss: 22062297199.7091 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 20680972027.3455 - accuracy: 0.0000e+00 - val_loss: 21972300371.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 281/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 20556569618.6182 - accuracy: 0.0000e+00 - val_loss: 21883199972.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 20435575789.3818 - accuracy: 0.0000e+00 - val_loss: 21793078402.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 34us/step - loss: 20312679368.1455 - accuracy: 0.0000e+00 - val_loss: 21703533996.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 20191043137.1636 - accuracy: 0.0000e+00 - val_loss: 21613931836.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/500\n",
      "880/880 [==============================] - 0s 39us/step - loss: 20070132233.3091 - accuracy: 0.0000e+00 - val_loss: 21524166320.8727 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 19946202838.1091 - accuracy: 0.0000e+00 - val_loss: 21435993404.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/500\n",
      "880/880 [==============================] - 0s 40us/step - loss: 19827793138.0364 - accuracy: 0.0000e+00 - val_loss: 21346552366.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 19704293152.5818 - accuracy: 0.0000e+00 - val_loss: 21258905562.7636 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 19586137255.5636 - accuracy: 0.0000e+00 - val_loss: 21170066860.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 19466643828.3636 - accuracy: 0.0000e+00 - val_loss: 21081334914.3273 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 19348298398.2545 - accuracy: 0.0000e+00 - val_loss: 20992195751.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 19228013251.4909 - accuracy: 0.0000e+00 - val_loss: 20903860894.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 19108504482.9091 - accuracy: 0.0000e+00 - val_loss: 20816152166.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/500\n",
      "880/880 [==============================] - 0s 76us/step - loss: 18988888510.8364 - accuracy: 0.0000e+00 - val_loss: 20729187718.9818 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 18872490207.4182 - accuracy: 0.0000e+00 - val_loss: 20641622462.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 18753893506.3273 - accuracy: 0.0000e+00 - val_loss: 20554968752.8727 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/500\n",
      "880/880 [==============================] - 0s 38us/step - loss: 18641326750.2545 - accuracy: 0.0000e+00 - val_loss: 20466563332.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/500\n",
      "880/880 [==============================] - 0s 37us/step - loss: 18522368781.9636 - accuracy: 0.0000e+00 - val_loss: 20379509369.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 18407076286.8364 - accuracy: 0.0000e+00 - val_loss: 20292467879.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/500\n",
      "880/880 [==============================] - 0s 40us/step - loss: 18289367077.2364 - accuracy: 0.0000e+00 - val_loss: 20206857979.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 301/500\n",
      "880/880 [==============================] - 0s 37us/step - loss: 18177996129.7455 - accuracy: 0.0000e+00 - val_loss: 20119811984.2909 - val_accuracy: 0.0000e+00\n",
      "Epoch 302/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 18060667028.9455 - accuracy: 0.0000e+00 - val_loss: 20034693790.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 303/500\n",
      "880/880 [==============================] - ETA: 0s - loss: 18231429120.0000 - accuracy: 0.0000e+ - 0s 34us/step - loss: 17948693504.0000 - accuracy: 0.0000e+00 - val_loss: 19949167448.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 304/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 17834441970.0364 - accuracy: 0.0000e+00 - val_loss: 19864559020.2182 - val_accuracy: 0.0000e+00\n",
      "Epoch 305/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 17722608770.3273 - accuracy: 0.0000e+00 - val_loss: 19779911828.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 306/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 17610996680.1455 - accuracy: 0.0000e+00 - val_loss: 19695142092.8000 - val_accuracy: 0.0000e+00\n",
      "Epoch 307/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 17499022280.1455 - accuracy: 0.0000e+00 - val_loss: 19611158453.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 308/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 17387145178.7636 - accuracy: 0.0000e+00 - val_loss: 19527878507.0545 - val_accuracy: 0.0000e+00\n",
      "Epoch 309/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 17277328961.1636 - accuracy: 0.0000e+00 - val_loss: 19444267305.8909 - val_accuracy: 0.0000e+00\n",
      "Epoch 310/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 17168775335.5636 - accuracy: 0.0000e+00 - val_loss: 19360317886.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 17058546483.2000 - accuracy: 0.0000e+00 - val_loss: 19276996421.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 312/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 16948948824.4364 - accuracy: 0.0000e+00 - val_loss: 19194678960.8727 - val_accuracy: 0.0000e+00\n",
      "Epoch 313/500\n",
      "880/880 [==============================] - 0s 36us/step - loss: 16843946617.0182 - accuracy: 0.0000e+00 - val_loss: 19111033167.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 16733990409.3091 - accuracy: 0.0000e+00 - val_loss: 19029047258.7636 - val_accuracy: 0.0000e+00\n",
      "Epoch 315/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 16624908995.4909 - accuracy: 0.0000e+00 - val_loss: 18948824119.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 316/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 16523388928.0000 - accuracy: 0.0000e+00 - val_loss: 18866450469.2364 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 16414984117.5273 - accuracy: 0.0000e+00 - val_loss: 18786066953.3091 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 16312117024.5818 - accuracy: 0.0000e+00 - val_loss: 18704837911.2727 - val_accuracy: 0.0000e+00\n",
      "Epoch 319/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 16204446701.3818 - accuracy: 0.0000e+00 - val_loss: 18625655044.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 320/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 16103017546.4727 - accuracy: 0.0000e+00 - val_loss: 18545518815.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 321/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 16000025171.7818 - accuracy: 0.0000e+00 - val_loss: 18465902592.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 322/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 15898448411.9273 - accuracy: 0.0000e+00 - val_loss: 18386587387.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 323/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 15796011529.3091 - accuracy: 0.0000e+00 - val_loss: 18307986115.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 324/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 15694355623.5636 - accuracy: 0.0000e+00 - val_loss: 18230226757.8182 - val_accuracy: 0.0000e+00\n",
      "Epoch 325/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 15595747290.7636 - accuracy: 0.0000e+00 - val_loss: 18152063944.1455 - val_accuracy: 0.0000e+00\n",
      "Epoch 326/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 15495981651.7818 - accuracy: 0.0000e+00 - val_loss: 18074462580.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 327/500\n",
      "880/880 [==============================] - 0s 33us/step - loss: 15399977797.8182 - accuracy: 0.0000e+00 - val_loss: 17995848126.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 328/500\n",
      "880/880 [==============================] - 0s 30us/step - loss: 15298062652.5091 - accuracy: 0.0000e+00 - val_loss: 17919942656.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 329/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 15202980547.4909 - accuracy: 0.0000e+00 - val_loss: 17843236417.1636 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 15107247141.2364 - accuracy: 0.0000e+00 - val_loss: 17766666835.7818 - val_accuracy: 0.0000e+00\n",
      "Epoch 331/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 15008380946.6182 - accuracy: 0.0000e+00 - val_loss: 17692270182.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 332/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 14914416900.6545 - accuracy: 0.0000e+00 - val_loss: 17617522017.7455 - val_accuracy: 0.0000e+00\n",
      "Epoch 333/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 14822089783.8545 - accuracy: 0.0000e+00 - val_loss: 17542249639.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 334/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 14727011495.5636 - accuracy: 0.0000e+00 - val_loss: 17468163090.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 335/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 14632762777.6000 - accuracy: 0.0000e+00 - val_loss: 17395403794.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 336/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 14542763566.5455 - accuracy: 0.0000e+00 - val_loss: 17321508156.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 337/500\n",
      "880/880 [==============================] - 0s 37us/step - loss: 14451577018.1818 - accuracy: 0.0000e+00 - val_loss: 17247875109.2364 - val_accuracy: 0.0000e+00\n",
      "Epoch 338/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 14358932982.6909 - accuracy: 0.0000e+00 - val_loss: 17175860317.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 339/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 14270882704.2909 - accuracy: 0.0000e+00 - val_loss: 17103106718.2545 - val_accuracy: 0.0000e+00\n",
      "Epoch 340/500\n",
      "880/880 [==============================] - 0s 31us/step - loss: 14180243009.1636 - accuracy: 0.0000e+00 - val_loss: 17031854228.9455 - val_accuracy: 0.0000e+00\n",
      "Epoch 341/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 14091099620.0727 - accuracy: 0.0000e+00 - val_loss: 16961232523.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 14004079671.8545 - accuracy: 0.0000e+00 - val_loss: 16890406576.8727 - val_accuracy: 0.0000e+00\n",
      "Epoch 343/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 13917392337.4545 - accuracy: 0.0000e+00 - val_loss: 16819932737.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 344/500\n",
      "880/880 [==============================] - 0s 32us/step - loss: 13831522006.1091 - accuracy: 0.0000e+00 - val_loss: 16749780545.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 345/500\n",
      "880/880 [==============================] - 0s 34us/step - loss: 13743472193.1636 - accuracy: 0.0000e+00 - val_loss: 16681521896.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 346/500\n",
      "880/880 [==============================] - 0s 35us/step - loss: 13662712441.0182 - accuracy: 0.0000e+00 - val_loss: 16611262910.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 347/500\n",
      "128/880 [===>..........................] - ETA: 0s - loss: 13806276608.0000 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-9220c802367e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist=model.fit(X,Ytrain,epochs=500,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
